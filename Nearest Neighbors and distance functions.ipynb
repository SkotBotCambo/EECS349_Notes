{
 "metadata": {
  "name": "",
  "signature": "sha256:14c9f1a5c8d0a0c786b71efb78e1999fdb480e2b2796d9701334a54407297842"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Nearest Neighbors\n",
      "* example case-based or instance-based learning\n",
      "    * instance-based is different in that there is no model without having the training data as a reference, unlike decision trees which only need the rules learned from the original data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Voronoi diagrams (get from slides)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making instance-based learning better\n",
      "* consider distance measure\n",
      "    * typically euclidian\n",
      "* the number of neighbors to consider\n",
      "    * K-nearest, what should K be?\n",
      "* weighting function\n",
      "* how to fit with neighbors : majority vote with nearest neighbors or weighting based on distance to k-neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Computational Complexity\n",
      "* Decision trees can be complete fairly quickly if the data can be described using a simple tree.\n",
      "\n",
      "### Training\n",
      "* KNN can be thought of training time of 0 or O(1)\n",
      "* Decision Trees computational complexity depends on the **attributes(A), the training examples(D), and the splits(S)**\n",
      "    * O(ADS)\n",
      "\n",
      "### Testing\n",
      "* O(DA)\n",
      "* Decision Trees are O(S) : depending on only O(S), very fast at test time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pros and Cons of KNN\n",
      "### Advantages\n",
      "* Fast training (\"lazy\" method)\n",
      "* Learn complex functions easily\n",
      "* Don't Lose Information\n",
      "\n",
      "### Disadvantages\n",
      "* slow at query time\n",
      "* lots of storage\n",
      "*  **Easily fooled by irrelevant attributes** -> **The curse of dimensionality**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Curse of Dimensionality\n",
      "** mind-blowing hypersphere enscribed in hypercube chalkboard experience**\n",
      "* take away : As n goes to infinite, nearest neighbor distance approaches farthest neighbor distance\n",
      "\n",
      "## How can we solve this for KNN?\n",
      "* Guess : feature selection...\n",
      "* Guess : weighting by furthest to nearest neighbor\n",
      "* dimensionality reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Selection\n",
      "* Pre-selection\n",
      "    * identify a good set of R features\n",
      "* Wrapping\n",
      "    * Starting with zero features, iterate: greedily add a new feature based on NN performance\n",
      "    * start at zero, look at all possible features and try an NN classifier, add feature that performs the best and stop when adding new features offers little benefit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Weighting Dimensions\n",
      "* Affects the shape of a classification region"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cutting Down on Computational Cost\n",
      "* Optimized Distance Computations\n",
      "    * Use cheap approximation techniques\n",
      "* Edited KNN\n",
      "    * For each x\n",
      "        * If x correctly classified by the data set, remove x from the data set\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How Do We Choose K for the K-Nearest Neighbor model?\n",
      "* Cross-validation : subsample and re-test with different values of K, choose the best one.\n",
      "    * Cross-Validation is important, because training accuracy for 1 nearest neighbor is 100% (which will not work for test accuracy), we can assume that with K being a little higher, it would not be much more representative.\n",
      "* Form prototypes\n",
      "    * reduced training data set to a series of \"prototypes\" which are the most representative and least redundant data points\n",
      "* Reduce noisy instances\n",
      "* **Kernel Regression**\n",
      "    * a distance measure: Scaled Euclidian\n",
      "    * Number of neighbors to consider: All of them\n",
      "    * a weighting function (weight close neighbors more than further ones)\n",
      "    * Fit with neighbors based on weighting function\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Measuring Distance (a requirement for NN algorithms)\n",
      "* Also important for clustering\n",
      "* Lp Norms\n",
      "    * Equation in Slides\n",
      "    * L1 norm = Manhattan Distance : p = 1\n",
      "    * L2 norm = Euclidian Distance : p = 2\n",
      "    * Hamming Distance : p = 1 and x_i, y_i in {0,1} "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}