{
 "metadata": {
  "name": "",
  "signature": "sha256:9df07cd85d5572e896a008aaf5ac42dc8b1e11551ad5e9836f44ace4517ee18d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Decision Trees 4/1, 4/3, & 4/6/2015"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Inductive Bias\n",
      "## Selection Bias\n",
      "use a restricted hypthosesis space\n",
      "* linear separators\n",
      "* 2-level decision trees\n",
      "\n",
      "## Preference bias\n",
      "use the whole concept space, but state a preference over concepts\n",
      "* lowest-degree polynomial that separates the data\n",
      "* shortest decision tree that fits the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dtl(X):\n",
      "    if len(x) == 0:\n",
      "        return X # I think\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from chalkboard\n",
      "#     A, B, C    <--- Column names\n",
      "X = [[0, 0, 0], # a\n",
      "     [0, 0, 1], # b\n",
      "     [1, 0, 0], # c\n",
      "     [1, 1, 0], # d\n",
      "     [1, 1, 1]] # e\n",
      "\n",
      "Y = [0, 0, 1, 1, 0] # f(A,B,C)\n",
      "\n",
      "#our inductive bias is that we want shorter trees\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Choosing an attribute to split on\n",
      "* split on attribute that gives more information or rather splits the data set such that there exists the least heterogeneity in each set after the split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Information\n",
      "Information allows me to answer questions\n",
      "\n",
      "The more I do not know about the answer when I start, the more information the answer contains\n",
      "\n",
      "Simple case : 1 bit (0 or 1) is answer to a question with prior (0.5, 0.5)\n",
      "\n",
      "Information in an answer when prior is (P1, ..., Pn)\n",
      "\n",
      "H( (P1, ..., Pn) = Sum ([1:n]) - P_i log_2 P_i"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## on chalkboard\n",
      "x  P(x)    Naive   Smarter(prefix encoding)\n",
      "A  0.5     00      0\n",
      "B  0.25    01      10\n",
      "C  0.125   10      110\n",
      "D  0.125   11      111\n",
      "\n",
      "ABADAACB -> 01001110011010 (14 bits instead of 16)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using Information\n",
      "p = number of positive examples\n",
      "n = number of negative examples\n",
      "H( (p / (p + n), n / (p + n) ) ) bits needed to classify an example\n",
      "\n",
      "### Patrons\n",
      "\"none\" --> n = 2, p = 0\n",
      "\"some\" --> n = 0, p = 4\n",
      "\"full\" --> n = 4, p = 2  (6/12[4/6log(2)4/6+2/6log(2) 2/6] = 0.459  *Heterogenous"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How do we know that hypothesis, h ,is close to the target function, f?\n",
      "* Use theorems of computational/statistical theory\n",
      "* predict on a new set of variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What the learning curve tells us\n",
      "Learning curve depends on\n",
      "* whether or not it's realizable\n",
      "    * realizable if can express target function\n",
      "    * unrealizable if it can't\n",
      "* redundant expressiveness (attributes are mostly irrelevant)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Overfitting\n",
      "Tailoring hypothesis or output function to closely to the training data making it less accurate in predicting on new outputs than it could be.\n",
      "\n",
      "The observed examples (training data) may really be the target function, f(x), + noise (due to unseen attributes)\n",
      "\n",
      "An overfit hypothesis is modeling the noise as much as the target function and its predictive accuracy is harmed accordingly\n",
      "\n",
      "Error can be due to\n",
      "* annotation error (wrong label)\n",
      "* mismeasurement of attributes (problems in feature extraction)\n",
      "* **Irrelevant** attributes\n",
      "* target function **not realizable** "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Avoiding Overfitting\n",
      "* stop splitting when information gain is low or when split is not statistically significant\n",
      "* Grow full tree and then **prune** it when done (HW 2)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reduced Error Pruning\n",
      "Split into *training* and *validation* set\n",
      "\n",
      "Do until further pruning is harmful:\n",
      "1. Evaluate impact *validation* set of pruning each possible node (plus those below it)\n",
      "2. Greedily remove ... *add from slides when posted*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#C4.5 Algorithm (a decision tree algo)\n",
      "* prunes tree after building to improve generality\n",
      "* allows missing attributes in examples\n",
      "* allowing continuous ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Handling Unknown Attribute Values\n",
      "* add a new value that represents \"missing\"\n",
      "    * difficulty is that it is hard to put on a continuous scale"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Selection\n",
      "## Learning Curve\n",
      "* Allows us to understand whether the current set of parameters will get better as we add more data or not\n",
      "\n",
      "## K-Fold Cross-Validation (K=10)\n",
      "* Every sample gets used once\n",
      "\n",
      "## Leave-One-Out Cross-Validation(LOO)\n",
      "* can be impractical because of how many iterations it might take to complete (if you have millions of data points, you will need millions of iterations)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Conclusion on Decision Trees\n",
      "* Used as classifiers\n",
      "* Supervised Learning\n",
      "* great when we think a small tree will be our target"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}