{
 "metadata": {
  "name": "",
  "signature": "sha256:0839111a83581517ce16214144084398135c94495d0f6f2630f2a4bc98db32bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Nearest Neighbors\n",
      "* example case-based or instance-based learning\n",
      "    * instance-based is different in that there is no model without having the training data as a reference, unlike decision trees which only need the rules learned from the original data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## voronoi diagrams (get from slides)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making instance-based learning better\n",
      "* consider distance measure\n",
      "    * typically euclidian\n",
      "* the number of neighbors to consider\n",
      "    * K-nearest, what should K be?\n",
      "* weighting function\n",
      "* how to fit with neighbors : majority vote with nearest neighbors or weighting based on distance to k-neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Computational Complexity\n",
      "* Decision trees can be complete fairly quickly if the data can be described using a simple tree.\n",
      "\n",
      "### Training\n",
      "* KNN can be thought of training time of 0 or O(1)\n",
      "* Decision Trees computational complexity depends on the **attributes(A), the training examples(D), and the splits(S)**\n",
      "    * O(ADS)\n",
      "\n",
      "### Testing\n",
      "* O(DA)\n",
      "* Decision Trees are O(S) : depending on only O(S), very fast at test time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pros and Cons of KNN\n",
      "### Advantages\n",
      "* Fast training (\"lazy\" method)\n",
      "* Learn complex functions easily\n",
      "* Don't Lose Information\n",
      "\n",
      "### Disadvantages\n",
      "* slow at query time\n",
      "* lots of storage\n",
      "*  **Easily fooled by irrelevant attributes** -> **The curse of dimensionality**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Curse of Dimensionality\n",
      "** mind-blowing hypersphere enscribed in hypercube chalkboard experience**\n",
      "* take away : As n goes to infinite, nearest neighbor distance approaches farthest neighbor distance\n",
      "\n",
      "## How can we solve this for KNN?\n",
      "* Guess : feature selection...\n",
      "* Guess : weighting by furthest to nearest neighbor\n",
      "* dimensionality reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Selection\n",
      "* Pre-selection\n",
      "    * identify a good set of R features\n",
      "* Wrapping\n",
      "    * Starting with zero features, iterate: greedily add a new feature based on NN performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}